{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f889fbca-93cc-4a70-a163-0a8961a8b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12789b5-1499-4893-b57d-384f289f70d8",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "    -- converting each word into numbers so that the computer can understand\n",
    "    -- Embedding layer takes each word and turns it into a small list of numbers (called vector) that captures the word's meaning\n",
    "    -- words that are similar or related will have vectors that look alike - this helps the computer to understand the connection b/w words\n",
    "\n",
    "### LSTM (Long Short-Term Memory)\n",
    "    -- it is aspecial knd of memory for computer to remember information from a sequence, like words in a sentence\n",
    "    -- just like a smart note-taker who remembers what's imp in a story even if it was said earlier\n",
    "\n",
    "### Bidirectional\n",
    "    -- reads the sequence twice (forward & backward)\n",
    "\n",
    "### Dense\n",
    "    -- it is the final decision-maker that combines everything that computer has learned and gives the final answer/prediction\n",
    "    -- in sentiment analysis - this review is +ve/-ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d57d6498-1953-47e2-b6ef-c4397e593f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download needed NLTK data\n",
    "nltk.data.path.append(r'C:\\Users\\tande\\AppData\\Roaming\\nltk_data')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3995b2b-7316-4649-85d8-21355849a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a6d2e04-92a6-47a0-82fb-6d7151796ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\tande\\OneDrive - Pyramid Foods\\sample data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ad3efa-27f4-4b6a-ad6d-1b1630d36826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning and tokenization with NLTK\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = tokenizer.tokenize(text.lower()) # changes letters into lower-case, word_tokenize = breaks the text into individual words\n",
    "    tokens = [t for t in tokens if t not in string.punctuation] # removes all punctuation marks (comma, period, exclamation)\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')] # removes common 'stop-words' - 'is', 'the', 'and', 'a', 'in'\n",
    "    return ' '.join(tokens) # putting remaining words back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a51cad0-028e-4f79-ba8a-5843cb7a708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_review'] = df['REVIEW'].apply(clean_text) # takes the 'review' column in dataframe, apply() -> run funtion on each review one-by-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3c32082-0d43-4d6e-9f4b-06862e1ecb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming text into token sequences with keras tokenizer\n",
    "max_words = 5000\n",
    "max_len = 20\n",
    "tokenizers = Tokenizer(num_words=max_words)\n",
    "tokenizers.fit_on_texts(df['clean_review'])\n",
    "sequences = tokenizers.texts_to_sequences(df['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06eed16b-866d-4678-81bf-4076015816f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the token sequences\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a522b1b0-1b1d-4f00-9b77-bc1026a806f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode target labels\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['RATING'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab338f91-d6d8-45a2-8e26-12bc6432926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df[['REVIEW', 'RATING']], test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ebcf49c3-9fa9-4628-98e8-d7e0ace80ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train['RATING']\n",
    "y_test = y_test['RATING']\n",
    "reviews_test = y_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e2cb5fab-d937-403b-9e30-5cc16c99af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buid and compile bidirectional LSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=32, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(6, activation='softmax')) # 0-5 classes for rating 1-5\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb3cb0c9-7b4a-4c78-ba1c-9e9a6cc8026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.2062 - loss: 1.7015\n",
      "Epoch 2/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4000 - loss: 1.5383\n",
      "Epoch 3/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7875 - loss: 1.0137\n",
      "Epoch 4/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9750 - loss: 0.3954\n",
      "Epoch 5/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1498\n",
      "Epoch 6/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0704\n",
      "Epoch 7/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0434\n",
      "Epoch 8/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0298\n",
      "Epoch 9/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0218\n",
      "Epoch 10/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2976be8dd90>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "324998b2-1f6c-49d0-aad6-ae23eedb372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss: 0.0182, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Evaluation Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f96d7cf-2954-4307-9561-a92b4b46a251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00         7\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00        14\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 7  0  0  0  0]\n",
      " [ 0  7  0  0  0]\n",
      " [ 0  0  7  0  0]\n",
      " [ 0  0  0  5  0]\n",
      " [ 0  0  0  0 14]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict class probabilities\n",
    "y_pred_probs = model.predict(X_test)\n",
    "# Convert probabilities to class labels (choose class with highest probability)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Classification report for detailed metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eceb76b9-5722-4884-b2f1-ca3b344d59be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "                                                Review  Actual Rating  \\\n",
      "95   absolutely fantastic product exceeded expectat...              5   \n",
      "15                                       loved it. buy              5   \n",
      "30   absolutely fantastic product exceeded expectat...              5   \n",
      "158                        mediocre experience overall              2   \n",
      "128                                  works major flaws              2   \n",
      "115  absolutely fantastic product exceeded expectat...              5   \n",
      "69                                horrible want refund              1   \n",
      "170                        works perfectly great value              5   \n",
      "174                    completely useless disappointed              1   \n",
      "45                                       loved it. buy              5   \n",
      "\n",
      "     Predicted Rating  \n",
      "95                  5  \n",
      "15                  5  \n",
      "30                  5  \n",
      "158                 2  \n",
      "128                 2  \n",
      "115                 5  \n",
      "69                  1  \n",
      "170                 5  \n",
      "174                 1  \n",
      "45                  5  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Predict class probabilities and convert to predicted class labels\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Create a DataFrame to compare\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Review': df.loc[y_test.index, 'clean_review'],  # original reviews in the test set\n",
    "    'Actual Rating': y_test,\n",
    "    'Predicted Rating': y_pred\n",
    "})\n",
    "\n",
    "# Show first 10 rows\n",
    "print(comparison_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847c261-9e0c-4ee2-ad06-6764f59ed23c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
